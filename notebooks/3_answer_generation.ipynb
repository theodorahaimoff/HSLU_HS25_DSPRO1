{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55ed98cc-234a-4724-af0e-e0afcc4f9e35",
   "metadata": {},
   "source": [
    "# Answering & Evaluation (Chroma → Ollama)\n",
    "\n",
    "**Goal:** Take retrieved legal articles (from Chroma) and generate a **grounded, structured answer** using a *local* Ollama model (e.g., `llama3:8b`).\n",
    "\n",
    "**What we’ll do:**\n",
    "1) Load the Chroma collection\n",
    "2) Retrieve top-K relevant articles (uses the helpers from Notebook 2)\n",
    "3) Build a clean context block with citations like `[OR Art. 269d – OR.pdf]`\n",
    "4) Call **Ollama HTTP API** locally to generate the answer\n",
    "5) Run a small evaluation set of typical user questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55bf20-f2f9-4ee1-a42f-c148f7896315",
   "metadata": {},
   "source": [
    "Imports & Paths"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf63ef16-8ba3-491d-80c0-eba0cc9eb018",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:24:52.044280Z",
     "start_time": "2025-11-07T00:24:47.494580Z"
    }
   },
   "source": [
    "import os, json, requests\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import chromadb, logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    # Jupyter fallback\n",
    "    BASE_DIR = Path(os.getcwd())\n",
    "\n",
    "CHROMA_DIR = (BASE_DIR.parent / \"store\").resolve()\n",
    "CHROMA_COLLECTION = \"swiss_private_rental_law\"\n",
    "\n",
    "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Retrieval knobs\n",
    "TOP_K  = 5\n",
    "PRE_K  = 20\n",
    "MAX_CTX_CHARS = 8000\n",
    "\n",
    "# Ollama local settings\n",
    "OLLAMA_HOST  = os.getenv(\"OLLAMA_HOST\", \"http://localhost:11434\")\n",
    "OLLAMA_MODEL = os.getenv(\"OLLAMA_MODEL\", \"llama3:8b\")\n",
    "\n",
    "logging.getLogger(\"chromadb\").setLevel(logging.DEBUG)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[0;93m2025-11-07 01:24:47.837061362 [W:onnxruntime:Default, device_discovery.cc:164 DiscoverDevicesForPlatform] GPU device discovery failed: device_discovery.cc:89 ReadFileContents Failed to open file: \"/sys/class/drm/card5/device/vendor\"\u001B[m\n",
      "/home/theodora/PycharmProjects/HSLU_HS25_DSPRO1/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "66481d3f-7563-4d1b-9baf-3c8e9ee77e7f",
   "metadata": {},
   "source": [
    "We verify:\n",
    "- Chroma store exists and is readable\n",
    "- The collection is present\n",
    "- Ollama is reachable and model is available\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c020c75-2b9f-4d5e-aa44-81a6f01eb2a6",
   "metadata": {},
   "source": [
    "Chroma & Embedder helpers (same logic as indexing_and_retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b9aa3b1-e8f6-43e8-9fca-8e76239a7035",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:24:52.052708Z",
     "start_time": "2025-11-07T00:24:52.049533Z"
    }
   },
   "source": [
    "# Disable analytics/telemetry\n",
    "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"false\"\n",
    "os.environ[\"POSTHOG_DISABLED\"] = \"true\"\n",
    "\n",
    "def get_client():\n",
    "    return chromadb.PersistentClient(path=str(CHROMA_DIR))\n",
    "\n",
    "def get_collection(name=CHROMA_COLLECTION):\n",
    "    client = get_client()\n",
    "    return client.get_collection(name)\n",
    "\n",
    "_embedder = None\n",
    "def embedder():\n",
    "    global _embedder\n",
    "    if _embedder is None:\n",
    "        _embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
    "    return _embedder\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "5dd82796-4a63-448a-9f3c-27757e98fa26",
   "metadata": {},
   "source": [
    "Check collection & doc count"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9d2e2df-5c06-4532-ba7c-15adfe11162c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:24:52.393598Z",
     "start_time": "2025-11-07T00:24:52.097649Z"
    }
   },
   "source": [
    "try:\n",
    "    col = get_collection()\n",
    "    print(\"Collection:\", CHROMA_COLLECTION, \"| count:\", col.count())\n",
    "except Exception as e:\n",
    "    raise SystemExit(f\"❌ Could not open Chroma collection. Did you run Notebook 2? Error: {e}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection: swiss_private_rental_law | count: 118\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "15556ea9-787a-4c3d-b6f3-10c8bbd55b2f",
   "metadata": {},
   "source": [
    "Check Ollama is running"
   ]
  },
  {
   "cell_type": "code",
   "id": "bae125f4-0776-4915-93d2-20f4b0b4aaa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.515542Z",
     "start_time": "2025-11-07T00:24:52.406730Z"
    }
   },
   "source": [
    "def check_ollama(host=OLLAMA_HOST, model=OLLAMA_MODEL):\n",
    "    try:\n",
    "        r = requests.get(host, timeout=5)\n",
    "        ok_base = r.status_code in (200, 404)  # / returns 404 often, that's fine if host reachable\n",
    "    except Exception as e:\n",
    "        return False, f\"Host not reachable: {e}\"\n",
    "\n",
    "    try:\n",
    "        # quick no-op generate to ensure model is present\n",
    "        test = requests.post(\n",
    "            f\"{host}/api/generate\",\n",
    "            json={\"model\": model, \"prompt\": \"OK\", \"stream\": False},\n",
    "            timeout=20\n",
    "        )\n",
    "        ok_model = (test.status_code == 200)\n",
    "        return ok_model, None if ok_model else f\"Model call failed: {test.text[:200]}\"\n",
    "    except Exception as e:\n",
    "        return False, f\"Model not available: {e}\"\n",
    "\n",
    "ok, err = check_ollama()\n",
    "print(\"Ollama ready:\", ok, \"| model:\", OLLAMA_MODEL)\n",
    "if not ok:\n",
    "    print(\"Hint: Run `ollama pull llama3:8b` and ensure Ollama is running.\")\n",
    "    if err: print(\"Details:\", err)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama ready: True | model: llama3:8b\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "73e0d9fc-9f7b-4721-abd4-5f54fff8cad0",
   "metadata": {},
   "source": [
    "We reuse a lightweight retrieval pipeline:\n",
    "- Embed the query\n",
    "- Query Chroma (optionally prefetch `PRE_K` and re-rank)\n",
    "- Format a **compact context** with clear citations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e590a5e-55c8-4f2d-88d0-ba768db43f33",
   "metadata": {},
   "source": [
    "Retrieve & (optional) re-rank + pack context"
   ]
  },
  {
   "cell_type": "code",
   "id": "a9c7752a-281f-4d67-97dd-57f69d968990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.593340Z",
     "start_time": "2025-11-07T00:25:00.587622Z"
    }
   },
   "source": [
    "def retrieve(query: str, k: int = TOP_K, k_pre: int = PRE_K, collection_name: str = CHROMA_COLLECTION):\n",
    "    col = get_collection(collection_name)\n",
    "    q_emb = embedder().encode([query], normalize_embeddings=True).tolist()[0]\n",
    "    res = col.query(query_embeddings=[q_emb], n_results=k_pre, include=['documents','metadatas','distances'])\n",
    "\n",
    "    docs  = res.get('documents', [[]])[0]\n",
    "    metas = res.get('metadatas', [[]])[0]\n",
    "    dists = res.get('distances', [[]])[0]\n",
    "    prelim = list(zip(docs, metas, dists))\n",
    "\n",
    "    # Optional: cross-encoder rerank (commented out; requires transformers/torch)\n",
    "    try:\n",
    "        from sentence_transformers import CrossEncoder\n",
    "        reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        scores = reranker.predict([(query, d) for d,_,_ in prelim]).tolist()\n",
    "        prelim = [p for p,_ in sorted(zip(prelim, scores), key=lambda x: x[1], reverse=True)]\n",
    "    except Exception:\n",
    "        prelim = sorted(prelim, key=lambda x: x[2])  # distance ascending\n",
    "\n",
    "    return prelim[:k]\n",
    "\n",
    "def pack_context(retrieved, max_chars=MAX_CTX_CHARS, per_source_cap=3):\n",
    "    \"\"\"\n",
    "    Build the context string and an id_map so we can later map used IDs back to metadata.\n",
    "    Returns: context_text, id_map (list of dicts with id, law, article, source)\n",
    "    \"\"\"\n",
    "    ctx, total, seen = [], 0, {}\n",
    "\n",
    "    for doc, meta, dist in retrieved:\n",
    "        key = (meta.get(\"law\"), meta.get(\"article\"))\n",
    "        seen[key] = seen.get(key, 0) + 1\n",
    "        if seen[key] > per_source_cap:\n",
    "            continue\n",
    "\n",
    "        stamp = f\"[{meta.get('law','?')} {meta.get('title','?')} – {meta.get('source')}]\"\n",
    "        block = f\"{stamp}\\n{doc.strip()}\\n\\n\"\n",
    "        if total + len(block) > max_chars:\n",
    "            break\n",
    "\n",
    "        ctx.append(block)\n",
    "        total += len(block)\n",
    "    return \"\".join(ctx)\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "84b79216-cced-44d5-b436-e225cb69dc8b",
   "metadata": {},
   "source": [
    "### Prompt design\n",
    "\n",
    "We force a strict structure for answers and **forbid** using anything outside the provided context.\n",
    "\n",
    "**Format required:**\n",
    "1) One-sentence answer.\n",
    "2) Numbered steps/options (say if they apply to Tenant or Landlord).\n",
    "3) Forms required (exact names if present).\n",
    "4) Articles to read next (e.g., Art. 269 OR; Art. 19 VMWG).\n",
    "\n",
    "Then **References** as `[LAW Art.X – filename]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c3d7d76-b087-4316-898e-48952d977dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.648241Z",
     "start_time": "2025-11-07T00:25:00.639982Z"
    }
   },
   "source": [
    "PROMPT = \"\"\"You are a Swiss rental-law assistant.\n",
    "Answer ONLY from the CONTEXT. If insufficient, say so.\n",
    "Do NOT refer to yourself, your role, or your identity in the answer.\n",
    "Start directly with the content requested (no introductions).\n",
    "\n",
    "FORMAT STRICTLY:\n",
    "1) \"**Antwort**:\" One concise sentence.\n",
    "2) \"**Schritte/Optionen**:\" NUMBERED points tailored to the given Perspective.\n",
    "3) \"**Formulare**:\" bullet list of exact official form names if present in CONTEXT, otherwise write \"Keine für diesen Fall gefunden.\"\n",
    "5) \"**Referenzen**:\" bullet list of distinct sources from CONTEXT as [law title – filename].\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def answer_with_ollama(question: str, perspective: str, k=TOP_K, model=OLLAMA_MODEL, host=OLLAMA_HOST):\n",
    "    \"\"\"\n",
    "    Query Ollama with retrieved context and return:\n",
    "    (generated_answer, used_references, hits)\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Retrieve documents\n",
    "    hits = retrieve(question, k=k)\n",
    "    context = pack_context(hits, max_chars=MAX_CTX_CHARS)\n",
    "\n",
    "    # 2. Generate answer\n",
    "    prompt = PROMPT.format(\n",
    "        context=context,\n",
    "        question=f\"Perspective: {perspective}, Question: {question}\"\n",
    "    )\n",
    "\n",
    "    # 3) Define the structured output schema\n",
    "    schema = {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"answer\": {\"type\": \"string\"},\n",
    "            \"steps\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\", \"maxLength\": 180},\n",
    "                \"minItems\": 2,\n",
    "                \"maxItems\": 8,\n",
    "                \"uniqueItems\": False\n",
    "            },\n",
    "            \"forms\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\"type\": \"string\"},\n",
    "                \"minItems\": 0,\n",
    "                \"maxItems\": 10\n",
    "            },\n",
    "            \"references\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"law\":     {\"type\": \"string\"},\n",
    "                        \"title\": {\"type\": \"string\"},\n",
    "                        \"source\":  {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"law\", \"title\", \"source\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"answer\", \"references\"]\n",
    "    }\n",
    "\n",
    "    # 4) Call Ollama /api/chat with schema-enforced output\n",
    "    r = requests.post(\n",
    "        f\"{host}/api/chat\",\n",
    "        json={\n",
    "            \"model\": model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt},\n",
    "                {\"role\": \"user\", \"content\":\n",
    "                    \"Beantworte die Frage gemäss obigen Vorgaben. \"\n",
    "                    \"Fülle die Felder des JSON-Schemas aus. \"\n",
    "                    \"Sprache: Deutsch (Schweiz), Du-Form. \"\n",
    "                    \"Für 'steps' gilt: Gib 2–8 kurze Einträge zurück, \"\n",
    "                    \"jeder Eintrag genau EIN Schritt, EINE Zeile, KEINE Nummerierung oder Zeilenumbrüche. \"\n",
    "                    \"Für 'forms': gib die genauen offiziellen Bezeichnungen aus dem CONTEXT zurück (leer, wenn keine). \"\n",
    "                    \"Gib NUR JSON zurück.\"\n",
    "                },\n",
    "            ],\n",
    "            \"stream\": False,\n",
    "            \"format\": schema,\n",
    "            \"options\": {\"temperature\": 0}\n",
    "        },\n",
    "        timeout=120\n",
    "    )\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return f\"[Ollama error {r.status_code}]: {r.text}\", hits\n",
    "\n",
    "    # 5) Parse the JSON content returned by /api/chat\n",
    "    data = r.json()\n",
    "    content = data.get(\"message\", {}).get(\"content\", \"\")\n",
    "    try:\n",
    "        parsed = json.loads(content) if isinstance(content, str) else content\n",
    "    except Exception as e:\n",
    "        return f\"[Parse error]: {e}\\nRaw: {content}\", [], hits\n",
    "\n",
    "    answer_text = (parsed.get(\"answer\") or \"\").strip()\n",
    "    steps = parsed.get(\"steps\") or []\n",
    "    if isinstance(steps, str):\n",
    "        steps = [s.strip() for s in steps.split(\"\\n\") if s.strip()]\n",
    "\n",
    "    forms = parsed.get(\"forms\") or []\n",
    "    if isinstance(forms, str):\n",
    "        forms = [f.strip() for f in forms.split(\"\\n\") if f.strip()]\n",
    "\n",
    "    references = parsed.get(\"references\") or []\n",
    "\n",
    "    return answer_text, steps, forms, references, hits\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a2d09e1c-5393-434c-9ff5-6d9049bf379f",
   "metadata": {},
   "source": [
    "Try a realistic query and inspect the sources retrieved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c031b75e-e2b1-4fba-8537-752bdc8db347",
   "metadata": {},
   "source": [
    "Single question test"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0ea2ea4-23d0-469b-a6ee-19ba10736c78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.697967Z",
     "start_time": "2025-11-07T00:25:00.693310Z"
    }
   },
   "source": [
    "def single_question_test():\n",
    "    q = \"Wie fechte ich eine Mietzinserhöhung an? Welches Formular ist nötig?\"\n",
    "    ans, hits = answer_with_ollama(q, perspective=\"Tenant\", k=6)\n",
    "    print(\"=== ANSWER ===\\n\", ans, \"\\n\")\n",
    "    print(\"=== SOURCES ===\")\n",
    "    for _, m, _ in hits:\n",
    "        print(f\"- {m.get('law')} Art.{m.get('article')} – {m.get('source')}\")"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.745100Z",
     "start_time": "2025-11-07T00:25:00.742535Z"
    }
   },
   "cell_type": "code",
   "source": "#single_question_test()",
   "id": "3a2241ddb82aacd",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "66991c55-6875-4543-bd7a-07bdb42a6d4e",
   "metadata": {},
   "source": [
    "We’ll run several canonical questions to check:\n",
    "- Structure & clarity of answers\n",
    "- That references point to the right law/articles\n",
    "- That forms are extracted when present (from VMWG, OR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b65fa32-a64c-4958-bf4f-a63d14ca5124",
   "metadata": {},
   "source": [
    "Batch evaluation"
   ]
  },
  {
   "cell_type": "code",
   "id": "e30b5afa-e1d9-47ec-b9b9-c06267bb8b21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.792171Z",
     "start_time": "2025-11-07T00:25:00.788508Z"
    }
   },
   "source": [
    "def batch_evaluation():\n",
    "    eval_questions = [\n",
    "        (\"Wie fechte ich eine Mietzinserhöhung an? Welches Formular ist nötig?\", \"Tenant\", \"English\"),\n",
    "        (\"Welche Rechte habe ich bei Mängeln in der Wohnung?\", \"Tenant\", \"German\"),\n",
    "        (\"Darf der Vermieter während laufendem Schlichtungsverfahren kündigen?\", \"Landlord\", \"English\"),\n",
    "        (\"Wann sind Mietzinserhöhungen wegen energetischer Verbesserungen zulässig?\", \"Landlord\", \"German\"),\n",
    "    ]\n",
    "\n",
    "    for q, perspective in eval_questions:\n",
    "        print(\"\\n\" + \"=\"*150)\n",
    "        print(\"Q:\", q, \"| Perspective:\", perspective)\n",
    "        print(\"=\"*150)\n",
    "        ans, hits = answer_with_ollama(q, perspective=perspective, k=6)\n",
    "        print(\"\\n--- ANSWER ---\\n\", ans[:2000])  # trim for display\n",
    "        print(\"\\n--- REFERENCES ---\")\n",
    "        refs = {(m.get('law'), m.get('article'), m.get('source')) for _, m, _ in hits}\n",
    "        for law, art, src in refs:\n",
    "            print(f\"[{law} Art.{art} – {src}]\")\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.847516Z",
     "start_time": "2025-11-07T00:25:00.841739Z"
    }
   },
   "cell_type": "code",
   "source": "#batch_evaluation()",
   "id": "f684cd2fa9688e4b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a723f95c-132a-4aae-8f7a-902b37b740e4",
   "metadata": {},
   "source": [
    "### Common issues & fixes\n",
    "\n",
    "- **`Collection … count: 0`**  \n",
    "  Run Notebook 2 (Indexing) first to build the Chroma collection.\n",
    "\n",
    "- **Ollama error / not reachable**  \n",
    "  Ensure Ollama is running and the model is available:  \n",
    "  `ollama serve` (if needed), then `ollama pull llama3:8b`.\n",
    "\n",
    "- **Answers not following format**  \n",
    "  Tighten the prompt (you can add: “If you deviate from the format, respond: ‘Insufficient’”).\n",
    "\n",
    "- **Irrelevant citations**  \n",
    "  Increase `k` or enable cross-encoder re-rank (install `transformers`, `torch`).\n",
    "\n",
    "- **Prefer a specific law**  \n",
    "  Add a `where={\"law\": \"OR\"}` filter in the `col.query(...)` call inside `retrieve()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbdc1b-e1c3-4628-a6c6-a211aad32676",
   "metadata": {},
   "source": [
    "# ✅ Wrap-up\n",
    "\n",
    "- Answers are now generated **locally** with Ollama using strictly the retrieved legal context.\n",
    "- Citations are explicit and article-level, boosting trust.\n",
    "- You can toggle perspective (“Tenant” / “Landlord”) to tailor steps.\n",
    "\n",
    "**Next (optional):** Build a tiny Streamlit UI (`app.py`) with a dropdown (Perspective), textbox (Question), and output panel (Answer + References).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b51a0f7b-163b-49c9-8779-f6525307f794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T00:25:00.901082Z",
     "start_time": "2025-11-07T00:25:00.896842Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
