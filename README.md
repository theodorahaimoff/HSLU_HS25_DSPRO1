# Rental Law RAG (Swiss OR / VMWG / StGB)

**What it does**  
Retrieval-Augmented QA for Swiss rental law. It splits PDFs into **per-article** JSON, builds a **Chroma** vector index, and answers questions with a **local Ollama** model (`llama3:8b`), citing `[LAW Art.X â€“ filename]`.

---

## Repo Layout
```bash
rental_law_rag/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ OR.pdf
â”‚   â”‚   â”œâ”€â”€ STGB.pdf
â”‚   â”‚   â””â”€â”€ VMWG.pdf
â”‚   â””â”€â”€ json/
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ store/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 0_installations.ipynb
â”‚   â”œâ”€â”€ 1_data_preparation.ipynb
â”‚   â”œâ”€â”€ 2_indexing_and_retrieval.ipynb
â”‚   â””â”€â”€ 3_answer_generation_local_ollama.ipynb
â””â”€â”€ src/
    â”œâ”€â”€ _0_installations.py
    â”œâ”€â”€ _1_data_preparation.py
    â”œâ”€â”€ _2_indexing_and_retrieval.py
    â”œâ”€â”€ _3_answer_generation.py
    â””â”€â”€ main_local_ollama.py
```



---

## Setup

```bash
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate
pip install -r requirements.txt


ollama serve            # in one terminal
ollama pull llama3:8b   # once
```

## Usage

### 1ï¸âƒ£ Add PDFs
Put the 3 law PDFs into `data/raw/`:
- `OR.pdf`
- `VMWG.pdf`
- `STGB.pdf`

### 2ï¸âƒ£ Install packages
Run **Notebook 0** (`0_installation.ipynb`) \
â†’ installs the packages needed for the next steps

### 3ï¸âƒ£ Build JSON dataset
Run **Notebook 1** (`1_data_preparation.ipynb`)  
â†’ generates per-article JSON files in `data/json/`.

### 4ï¸âƒ£ Build Chroma index
Run **Notebook 2** (`2_indexing_and_retrieval.ipynb`)  
â†’ creates the persistent database in `store/`.

### 5ï¸âƒ£ Ask questions
Run **Notebook 3** (`3_answering_and_evaluation.ipynb`)  
â†’ asks Ollama locally (model `llama3:8b`) and prints legal answers with citations.

### 6ï¸âƒ£ Run Streamlit app
Run the following command on your terminal
```bash
streamlit run src/main_local_ollama.py
```
The application's GUI should now be available under http://localhost:8501/

## Notes for Collaborators
- Generated folders (`data/json/`, `store/`) are **git-ignored** â€” everyone rebuilds them locally.
- If Ollama isnâ€™t running, start it using:
  ```bash
  ollama serve
  ollama pull llama3:8b
  ```
- After editing any of the notebooks, generate the respective Python script:
  ```bash
  jupyter nbconvert --to script notebooks/1_data_preparation.ipynb --output "_1_data_preparation.py" --output-dir=src
  ```
  > ðŸ‘‰ **Note** \
  > Make sure you keep the _ character in front of the output file name, Python has issues when a script begins with a number